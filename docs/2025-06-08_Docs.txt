# ğŸ§¾ Documentation â€” 2025-06-08

## âœ… Objective:

Finalize the project structure and restore full working versions of `s3` and `s3b` (scraper + resume scraper) while restructuring pathing, input/output naming, and ensuring pipeline compatibility.

---

## ğŸ” 1. **Global Restructuralization**

### ğŸ—‚ï¸ New Folder Layout Standard:

```bash
ScraperML/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 1_extract/
â”‚   â”œâ”€â”€ 2_scrape/
â”‚   â”œâ”€â”€ 3_filter_clean/
â”‚   â”œâ”€â”€ 6_feature_engineering/
â”‚   â”œâ”€â”€ 7_location_category/
â”‚   â””â”€â”€ 8_merge/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ debug/
â”œâ”€â”€ mappings/
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tools/
â”œâ”€â”€ config.py
```

* **All I/O moved through `config.py`** constants:

  * `RAW_DATA_DIR`
  * `PROCESSED_DATA_DIR`
  * `DEBUG_DIR`

---

## ğŸ“¥ 2. **Input/Output Pathing Rework**

### âœ… All scripts now:

* Use full paths: `RAW_DATA_DIR / "file.csv"`
* Respect strict inputâ†’output flow
* Save to correct folder:

  * `data/raw/` â†’ for scraped/unprocessed
  * `data/processed/` â†’ for cleaned, filtered
  * `data/debug/` â†’ for exploratory / intermediate

### âœ… Output file naming:

Refactored to always reflect **their origin step**:

Examples:

* `s3_active_listings_with_prices_and_features.csv`
* `s4_active_cleanable_listings.csv`
* `s6b_field_frequency_debug.csv`

This ensures full **traceability**: when you see a file, you know where it came from.

---

## ğŸ 3. **Major Flaws Identified and Solved**

### âŒ Flaw 1: Rewritten `s3` broke parsing logic

* Later versions introduced vague HTML matching
* Output looked complete but contained **garbage**
* `"Base Price"` scraped random text
* `"Raw Characteristics"` were `{}` or invalid JSON

âœ… **Fix**: Rewinded to original hand-built version (`beta`) with exact DOM pathing

---

### âŒ Flaw 2: Resume logic failed silently

* URLs were not normalized (`strip()`, `rstrip("/")`)
* Resume scripts re-scraped everything again
* No output showed how many listings were skipped

âœ… **Fix**:

* Normalization applied to both input + existing URLs
* Print summary:

  ```
  ğŸ” Total: 16842 | âœ… Already scraped: 10420 | ğŸš§ Remaining: 6422
  ```

---

### âŒ Flaw 3: Misleading fast scrapes

* A broken `s3` ran "fast" (\~1.5h) because it wasnâ€™t scraping real data
* Output structure was valid-looking, but unusable in filters (`s4`, `s5`)
* All prices and characteristics were garbage â†’ filters found "nothing"

âœ… **Fix**: Restored original tag-walking parsing logic
âœ… `s4`, `s5`, `s6a`, `s6b` now function properly again

---

## ğŸ› ï¸ 4. **Final Working Scripts**

| Script                                            | Description                                      |
| ------------------------------------------------- | ------------------------------------------------ |
| `s3_ScrapeAllDataFromLinks_restored.py`           | âœ… Full working restored version of main scraper  |
| `s3b_ResumeScrap_restored.py`                     | âœ… Resume version that skips already scraped URLs |
| Both use `config.py` and correct output structure |                                                  |

---

## ğŸ”’ 5. **Frozen Reference Dataset**

`active_listings_with_prices_and_features_old_working.csv` was used as the **source of truth** for verifying:

* Column order
* Price and characteristics structure
* Valid fallback values (`"N/A"`, `"{}"`)

Now restored in output of `s3_restored`.

---

## âœ… Summary of Completed Actions

* âœ… All script folders moved to new structure
* âœ… I/O pathing and naming standardized
* âœ… `s3` scraper fully restored from working DOM structure
* âœ… Resume logic fixed and verified
* âœ… Pipeline validated: `s3 â†’ s4 â†’ s5 â†’ s6a â†’ s6b` now stable
* âœ… Garbage output issue diagnosed and prevented
* âœ… Print-based progress monitoring added

---



