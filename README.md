# ğŸ—ï¸ ScraperML â€” Real Estate Listing Extraction & Processing Pipeline

**ScraperML** is a modular data pipeline designed to:
- Scrape real estate listings
- Filter and clean scraped data
- Extract structured features
- Enrich metadata (location, category, etc.)
- Output a final clean dataset for analysis or machine learning

---

## ğŸ“‚ Project Structure

```
ScraperML/
â”œâ”€â”€ main.py                      # Runs full real pipeline
â”œâ”€â”€ extras/
â”‚   â””â”€â”€ main_dummy.py           # Runs pipeline with dummy s2, s3, s3b
â”‚
â”œâ”€â”€ config.py                   # Path configuration
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ debug/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 1_extract/
â”‚   â”‚   â”œâ”€â”€ s1_ExtractAllLinksToTxt.py
â”‚   â”‚   â”œâ”€â”€ s2_FilterActiveInactive.py
â”‚   â”‚   â””â”€â”€ extras/
â”‚   â”‚       â””â”€â”€ s2_FilterActiveInactive_dummie.py
â”‚
â”‚   â”œâ”€â”€ 2_scrape/
â”‚   â”‚   â”œâ”€â”€ s3_ScrapeAllDataFromLinks_restored.py
â”‚   â”‚   â”œâ”€â”€ s3b_ResumeScrap_restored.py
â”‚   â”‚   â””â”€â”€ extras/
â”‚   â”‚       â”œâ”€â”€ s3_ScrapeAllDataFromLinks_dummie.py
â”‚   â”‚       â””â”€â”€ s3b_ResumeScrap_dummie.py
â”‚
â”‚   â”œâ”€â”€ 3_filter_clean/
â”‚   â”‚   â”œâ”€â”€ s4_Filter2InactiveListings.py
â”‚   â”‚   â””â”€â”€ s5_CleanPriceAndFinalFilter.py
â”‚
â”‚   â”œâ”€â”€ 4_parse_features/
â”‚   â”‚   â”œâ”€â”€ s6a_ParseAndValidateRawChar.py
â”‚   â”‚   â”œâ”€â”€ s6b_ExtractStructuredFeatures.py
â”‚   â”‚   â””â”€â”€ s6c_AddListingID_viaRawCharacteristics.py
â”‚
â”‚   â”œâ”€â”€ 5_enrich/
â”‚   â”‚   â”œâ”€â”€ s7_MainExtractCategoryAndLocationFromTitle.py
â”‚   â”‚   â”œâ”€â”€ s7a_CleanCategoryLocationByID.py
â”‚   â”‚   â””â”€â”€ s7aa_CleanEngineeredListingsByID.py
â”‚
â”‚   â””â”€â”€ 6_merge/
â”‚       â””â”€â”€ s8_final_merged_dataset.py
â”‚
â”œâ”€â”€ mappings/
â”‚   â”œâ”€â”€ s6Bb_field_aliases.py
â”‚   â””â”€â”€ s7_title_category_map.py
â”‚
â””â”€â”€ utils/tools/
```

---

## âš™ï¸ Requirements

Install all required packages using:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running the Pipeline

### â–¶ï¸ Full production run

```bash
python main.py
```

### ğŸ§ª Dev/test run with dummy scripts

```bash
python extras/main_dummy.py
```

---

## ğŸ” Pipeline Step Summary

| Step | Script | Description |
|------|--------|-------------|
| `s1` | `s1_ExtractAllLinksToTxt.py` | Extract links from sitemap |
| `s2` | `s2_FilterActiveInactive.py` | Check link availability |
| `s3` | `s3_ScrapeAllDataFromLinks_restored.py` | Scrape listing content |
| `s3b` | `s3b_ResumeScrap_restored.py` | Resume missed listings |
| `s4` | `s4_Filter2InactiveListings.py` | Filter out inactive listings |
| `s5` | `s5_CleanPriceAndFinalFilter.py` | Clean and parse price fields |
| `s6a` | `s6a_ParseAndValidateRawChar.py` | Normalize raw features |
| `s6b` | `s6b_ExtractStructuredFeatures.py` | Extract structured features |
| `s6c` | `s6c_AddListingID_viaRawCharacteristics.py` | Assign consistent IDs |
| `s7` | `s7_MainExtractCategoryAndLocationFromTitle.py` | Extract category and location |
| `s7a` | `s7a_CleanCategoryLocationByID.py` | Clean and deduplicate |
| `s7aa` | `s7aa_CleanEngineeredListingsByID.py` | Finalize listing metadata |
| `s8` | `s8_final_merged_dataset.py` | Merge to one dataset |

---

## ğŸ“¦ Output

The final cleaned dataset will be saved as:

```
data/processed/s8_final_merged_dataset.csv
```

---

## ğŸ§  Notes

- All intermediate files are stored in `data/raw/`, `data/processed/`, and `data/debug/`.
- Dummy scripts (`s2`, `s3`, `s3b`) live in their respective `extras/` folders inside each module.
- Configuration is centralized via `config.py`.
- All file paths use `pathlib` for cross-platform safety.

---

## ğŸ“„ License

MIT â€” Free for personal, academic, or commercial use.
